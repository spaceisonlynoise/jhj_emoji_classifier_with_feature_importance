{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r jhj_emoji_texts\n",
    "%store -r jhj_emoji_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "858"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jhj_emoji_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######clean#######\n",
    "from num2words import num2words\n",
    "#convert everything to lowercase\n",
    "jhj_emoji_texts = [x.lower() for x in jhj_emoji_texts]\n",
    "#exchange numbers with their string representation\n",
    "#make numbers to text\n",
    "temp = []\n",
    "without_num = []\n",
    "for post in jhj_emoji_texts:\n",
    "    for word in post.split(\" \"):\n",
    "        if word.isdigit():\n",
    "            #print(word)\n",
    "            word = num2words(word, lang=\"de\")\n",
    "        temp.append(word)\n",
    "    without_num.append((' '.join(temp)))\n",
    "    temp = []\n",
    "jhj_emoji_texts = without_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######feature extraction#########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without deleting stopwords\n",
    "#ngram feature matrices\n",
    "\n",
    "#unigram\n",
    "uni_vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "unigram_feature_vector = uni_vectorizer.fit_transform(jhj_emoji_texts).toarray() \n",
    "\n",
    "#uni and bigram\n",
    "uni_bi_vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "unigram_bi_feature_vector = uni_bi_vectorizer.fit_transform(jhj_emoji_texts).toarray() \n",
    "\n",
    "#bigram\n",
    "bi_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "bigram_feature_vector = bi_vectorizer.fit_transform(jhj_emoji_texts).toarray()\n",
    "\n",
    "\n",
    "#trigram\n",
    "tri_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "trigram_feature_vector = tri_vectorizer.fit_transform(jhj_emoji_texts).toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00 00',\n",
       " '00 uhr',\n",
       " '00 was',\n",
       " '000 zahle',\n",
       " '08 15',\n",
       " '10 10',\n",
       " '10 daddy',\n",
       " '10 flixbus',\n",
       " '10 geschenk',\n",
       " '10 jahren',\n",
       " '10 januar',\n",
       " '10 karstadtistzuteuer',\n",
       " '10 lauter',\n",
       " '10 mal',\n",
       " '10 umzugskartons',\n",
       " '100 ausspuckt',\n",
       " '100 deren',\n",
       " '100 oder',\n",
       " '100 versaufen',\n",
       " '12 2019',\n",
       " '12 von',\n",
       " '1200 zur',\n",
       " '13 dachte',\n",
       " '13 jährigen',\n",
       " '14 30',\n",
       " '14 steigt',\n",
       " '15 alles',\n",
       " '15 bis',\n",
       " '15 dass',\n",
       " '15 gutscheine',\n",
       " '15 keinen',\n",
       " '15 soviel',\n",
       " '15 typ',\n",
       " '16qm am',\n",
       " '17 15',\n",
       " '17 20',\n",
       " '18 00',\n",
       " '18 19',\n",
       " '19 35',\n",
       " '19 auch',\n",
       " '19 jhd',\n",
       " '19h und',\n",
       " '20 abzugeben',\n",
       " '20 am',\n",
       " '20 frauen',\n",
       " '20 vom',\n",
       " '2000 netto',\n",
       " '200k einwohnern',\n",
       " '2019 haben',\n",
       " '21 schwarz',\n",
       " '21h noch',\n",
       " '22 uhr',\n",
       " '24 habe',\n",
       " '24 ungeküsst',\n",
       " '24 vietnamesin',\n",
       " '25 beste',\n",
       " '27 eine',\n",
       " '28 leicester',\n",
       " '29 beste',\n",
       " '2h sry',\n",
       " '2pdfs weiß',\n",
       " '2x angerufen',\n",
       " '30 12',\n",
       " '30 juni',\n",
       " '30 nicht',\n",
       " '30 schenken',\n",
       " '30 zeit',\n",
       " '33 uhr',\n",
       " '35 000',\n",
       " '35 überwiesen',\n",
       " '40 bezahlen',\n",
       " '40personen oder',\n",
       " '46 ghz',\n",
       " '4b himbeere',\n",
       " '50 die',\n",
       " '50 versprochen',\n",
       " '500 600',\n",
       " '500 sechshundert',\n",
       " '50cent für',\n",
       " '5h dauer',\n",
       " '5h gelernt',\n",
       " '60fps stable',\n",
       " '65t gemacht',\n",
       " '70 oder',\n",
       " '700 ich',\n",
       " '70ern gefunden',\n",
       " '77 ski',\n",
       " '79h mit',\n",
       " '7er losing',\n",
       " '7kk jh97',\n",
       " '80 aber',\n",
       " '80er jahre',\n",
       " '83 und',\n",
       " '89kg zu',\n",
       " '8k und',\n",
       " '8km zu',\n",
       " '8m zu',\n",
       " '94 kg',\n",
       " '95 quote',\n",
       " 'ab beginnt',\n",
       " 'ab heute',\n",
       " 'ab nachmittag',\n",
       " 'ab und',\n",
       " 'ab von',\n",
       " 'ab welcher',\n",
       " 'abbiege spur',\n",
       " 'abend bei',\n",
       " 'abend betrifft',\n",
       " 'abend rot',\n",
       " 'abend so',\n",
       " 'abend spieleabend',\n",
       " 'abend zum',\n",
       " 'abendessen bei',\n",
       " 'abends immer',\n",
       " 'abends weg',\n",
       " 'aber alle',\n",
       " 'aber an',\n",
       " 'aber anfänger',\n",
       " 'aber antwortet',\n",
       " 'aber auch',\n",
       " 'aber aussiehst',\n",
       " 'aber bin',\n",
       " 'aber bzgl',\n",
       " 'aber dann',\n",
       " 'aber das',\n",
       " 'aber der',\n",
       " 'aber die',\n",
       " 'aber dieses',\n",
       " 'aber doch',\n",
       " 'aber du',\n",
       " 'aber eben',\n",
       " 'aber ein',\n",
       " 'aber einen',\n",
       " 'aber einfach',\n",
       " 'aber er',\n",
       " 'aber es',\n",
       " 'aber flirte',\n",
       " 'aber fühle',\n",
       " 'aber gerne',\n",
       " 'aber gibt',\n",
       " 'aber hab',\n",
       " 'aber habe',\n",
       " 'aber ich',\n",
       " 'aber ihn',\n",
       " 'aber im',\n",
       " 'aber immer',\n",
       " 'aber in',\n",
       " 'aber jan',\n",
       " 'aber keinen',\n",
       " 'aber komplett',\n",
       " 'aber körperselfie',\n",
       " 'aber leckere',\n",
       " 'aber leider',\n",
       " 'aber manchmal',\n",
       " 'aber mega',\n",
       " 'aber mein',\n",
       " 'aber mit',\n",
       " 'aber möchte',\n",
       " 'aber nach',\n",
       " 'aber nicht',\n",
       " 'aber nichts',\n",
       " 'aber opfer',\n",
       " 'aber preislich',\n",
       " 'aber riecht',\n",
       " 'aber scheinbar',\n",
       " 'aber sie',\n",
       " 'aber sooooo',\n",
       " 'aber strikt',\n",
       " 'aber trotzdem',\n",
       " 'aber uns',\n",
       " 'aber was',\n",
       " 'aber wenigstens',\n",
       " 'aber wie',\n",
       " 'aber wieder',\n",
       " 'aber will',\n",
       " 'aber würde',\n",
       " 'aber zählt',\n",
       " 'abfolge manchmal',\n",
       " 'abfuhr bekommen',\n",
       " 'abgabe der',\n",
       " 'abgang gemacht',\n",
       " 'abgearbeitet haben',\n",
       " 'abgeben für',\n",
       " 'abgeben würdet',\n",
       " 'abgebrochen ist',\n",
       " 'abgefuckten schlafrhythmus',\n",
       " 'abgegeben du',\n",
       " 'abgeschleppt habichwohlzulangegeparkt',\n",
       " 'abgesperrten bereich',\n",
       " 'abgestumpfter ich',\n",
       " 'abiturient ernsthaft',\n",
       " 'ablaufdatum 12',\n",
       " 'ablehnen beenden',\n",
       " 'ablehnt rip',\n",
       " 'abnehmen wird',\n",
       " 'abonnenten verhältnis',\n",
       " 'abonniert zu',\n",
       " 'abos auf',\n",
       " 'abschalten um',\n",
       " 'abschicken was',\n",
       " 'absolut großartig',\n",
       " 'absolut kein',\n",
       " 'absolut keine',\n",
       " 'absolut keinen',\n",
       " 'absolut snejoejdsnbdjr',\n",
       " 'absolut surreal',\n",
       " 'absoluten lieblingsbeschäftigungen',\n",
       " 'abstieg zu',\n",
       " 'abteil hinter',\n",
       " 'abzockern unbedingt',\n",
       " 'abzugeben bei',\n",
       " 'abzugeben da',\n",
       " 'abzulenken tipps',\n",
       " 'abzüglich meiner',\n",
       " 'accessoires von',\n",
       " 'acer oh',\n",
       " 'ach und',\n",
       " 'achalmbad auch',\n",
       " 'achillessehne bzw',\n",
       " 'achja ich',\n",
       " 'acht stunden',\n",
       " 'acht zu',\n",
       " 'achtzehn jährigen',\n",
       " 'achtzehn sind',\n",
       " 'achtzig arbeitstunden',\n",
       " 'adern am',\n",
       " 'adhs für',\n",
       " 'adresse oder',\n",
       " 'adventskalender kaufen',\n",
       " 'afd auf',\n",
       " 'afrika fährt',\n",
       " 'aftersexselfie channel',\n",
       " 'ahhh man',\n",
       " 'ahnung davon',\n",
       " 'ahnung hat',\n",
       " 'ahnung von',\n",
       " 'ahnung was',\n",
       " 'ahnung wie',\n",
       " 'aiagwrecner sie',\n",
       " 'akne ganz',\n",
       " 'akzeptiert mich',\n",
       " 'alben von',\n",
       " 'albtraum nach',\n",
       " 'aldi gibt',\n",
       " 'alexa das',\n",
       " 'alexa du',\n",
       " 'alexa weihnachtensmusik',\n",
       " 'alexanderplatz noch',\n",
       " 'alkohol party',\n",
       " 'alkohol wenn',\n",
       " 'all my',\n",
       " 'all zu',\n",
       " 'alle an',\n",
       " 'alle auf',\n",
       " 'alle bitte',\n",
       " 'alle blau',\n",
       " 'alle die',\n",
       " 'alle dokus',\n",
       " 'alle ein',\n",
       " 'alle entspannten',\n",
       " 'alle erfahren',\n",
       " 'alle frauen',\n",
       " 'alle gegen',\n",
       " 'alle keinen',\n",
       " 'alle meine',\n",
       " 'alle perspektiven',\n",
       " 'alle richtungen',\n",
       " 'alle sagen',\n",
       " 'alle silvester',\n",
       " 'alle sind',\n",
       " 'alle so',\n",
       " 'alle tot',\n",
       " 'alle unsere',\n",
       " 'alle vergeben',\n",
       " 'alle zusammen',\n",
       " 'allein bin',\n",
       " 'allein daheim',\n",
       " 'allein krank',\n",
       " 'allein sein',\n",
       " 'alleine an',\n",
       " 'alleine bleiben',\n",
       " 'alleine dort',\n",
       " 'alleine muss',\n",
       " 'alleine raus',\n",
       " 'allem ab',\n",
       " 'allem ungewöhnlich',\n",
       " 'allen entschuldigen',\n",
       " 'allen nachbarn',\n",
       " 'allen religionen',\n",
       " 'aller häufungspunkte',\n",
       " 'aller zeiten',\n",
       " 'allerdings als',\n",
       " 'alles abgearbeitet',\n",
       " 'alles andere',\n",
       " 'alles geblockt',\n",
       " 'alles gerne',\n",
       " 'alles gut',\n",
       " 'alles halb',\n",
       " 'alles hat',\n",
       " 'alles in',\n",
       " 'alles kacke',\n",
       " 'alles kasssaaalllaaaaa',\n",
       " 'alles ok',\n",
       " 'alles passt',\n",
       " 'alles schläft',\n",
       " 'alles wird',\n",
       " 'alles wtf',\n",
       " 'alles zu',\n",
       " 'allgäu drin',\n",
       " 'allgäu nach',\n",
       " 'allmountain ski',\n",
       " 'alpträume stranger',\n",
       " 'als 40',\n",
       " 'als ablaufdatum',\n",
       " 'als achtzig',\n",
       " 'als benachrichtungston',\n",
       " 'als bild',\n",
       " 'als cd',\n",
       " 'als cuck',\n",
       " 'als dafür',\n",
       " 'als der',\n",
       " 'als einer',\n",
       " 'als einzige',\n",
       " 'als es',\n",
       " 'als freundin',\n",
       " 'als fährt',\n",
       " 'als geschenk',\n",
       " 'als gesundheitskauffrau',\n",
       " 'als große',\n",
       " 'als hätte',\n",
       " 'als ich',\n",
       " 'als instrumente',\n",
       " 'als jähriger',\n",
       " 'als letztes',\n",
       " 'als man',\n",
       " 'als mod',\n",
       " 'als musiktherapeut',\n",
       " 'als ob',\n",
       " 'als online',\n",
       " 'als romantisch',\n",
       " 'als sein',\n",
       " 'als sonst',\n",
       " 'als student',\n",
       " 'als verstand',\n",
       " 'als werkstudent',\n",
       " 'als wär',\n",
       " 'also auch',\n",
       " 'also avatar',\n",
       " 'also danke',\n",
       " 'also entweder',\n",
       " 'also etwas',\n",
       " 'also für',\n",
       " 'also hab',\n",
       " 'also kann',\n",
       " 'also nicht',\n",
       " 'also so',\n",
       " 'also suche',\n",
       " 'also von',\n",
       " 'alt bist',\n",
       " 'alt und',\n",
       " 'alte jahr',\n",
       " 'alten alben',\n",
       " 'alten alle',\n",
       " 'alten schulfreunden',\n",
       " 'alter apasche',\n",
       " 'alter hat',\n",
       " 'alter schmeckt',\n",
       " 'alter was',\n",
       " 'aluhut ojs',\n",
       " 'am 30',\n",
       " 'am anfang',\n",
       " 'am arsch',\n",
       " 'am besten',\n",
       " 'am ehesten',\n",
       " 'am elektronik',\n",
       " 'am ende',\n",
       " 'am hals',\n",
       " 'am handgelenk',\n",
       " 'am handy',\n",
       " 'am hart',\n",
       " 'am huastnguadl',\n",
       " 'am karlsplatz',\n",
       " 'am laufen',\n",
       " 'am liebsten',\n",
       " 'am meisten',\n",
       " 'am mittelfinger',\n",
       " 'am nonnenhaus',\n",
       " 'am nächsten',\n",
       " 'am pissoir',\n",
       " 'am rad',\n",
       " 'am schalter',\n",
       " 'am schlimmsten',\n",
       " 'am schnellsten',\n",
       " 'am sendlinger',\n",
       " 'am sonntag',\n",
       " 'am still',\n",
       " 'am tag',\n",
       " 'am verzweifeln',\n",
       " 'am vormittag',\n",
       " 'am wenigsten',\n",
       " 'am winter',\n",
       " 'am zoo',\n",
       " 'am überlegen',\n",
       " 'amazon bekomme',\n",
       " 'amazon mir',\n",
       " 'amazon prime',\n",
       " 'amsterdam und',\n",
       " 'an alle',\n",
       " 'an alles',\n",
       " 'an bars',\n",
       " 'an computer',\n",
       " 'an dass',\n",
       " 'an den',\n",
       " 'an der',\n",
       " 'an die',\n",
       " 'an einem',\n",
       " 'an einer',\n",
       " 'an fasching',\n",
       " 'an gerne',\n",
       " 'an haarausfall',\n",
       " 'an holo',\n",
       " 'an hässlichen',\n",
       " 'an ihn',\n",
       " 'an in',\n",
       " 'an kliche',\n",
       " 'an meinem',\n",
       " 'an meinen',\n",
       " 'an meiner',\n",
       " 'an mich',\n",
       " 'an mit',\n",
       " 'an männern',\n",
       " 'an münchen',\n",
       " 'an partys',\n",
       " 'an silvester',\n",
       " 'an skinoren',\n",
       " 'an stellen',\n",
       " 'an und',\n",
       " 'an weihnachten',\n",
       " 'an weiß',\n",
       " 'an wenn',\n",
       " 'anal ausprobiert',\n",
       " 'anal war',\n",
       " 'anblick eines',\n",
       " 'and am',\n",
       " 'and die',\n",
       " 'and jerry',\n",
       " 'and leave',\n",
       " 'and me',\n",
       " 'and you',\n",
       " 'andere 80er',\n",
       " 'andere beziehung',\n",
       " 'andere frauen',\n",
       " 'andere sitten',\n",
       " 'andere stadt',\n",
       " 'andere tricks',\n",
       " 'andere video',\n",
       " 'andere was',\n",
       " 'andere zeiten',\n",
       " 'andere zu',\n",
       " 'anderen auf',\n",
       " 'anderen ist',\n",
       " 'anderen kommilitonen',\n",
       " 'anderen menschen',\n",
       " 'anderen städten',\n",
       " 'anderen subkulturen',\n",
       " 'anderen tag',\n",
       " 'anderen umgebung',\n",
       " 'anderen und',\n",
       " 'anderer jodler',\n",
       " 'anderes level',\n",
       " 'anders abgespeichert',\n",
       " 'anfang des',\n",
       " 'anfang etwas',\n",
       " 'anfang nicht',\n",
       " 'anfang zwanzig',\n",
       " 'anfangen der',\n",
       " 'anfangen habe',\n",
       " 'anfänger im',\n",
       " 'angeblichen randalierer',\n",
       " 'angeblicher sensibilität',\n",
       " 'angeboten hat',\n",
       " 'angefangen hat',\n",
       " 'angefangen weil',\n",
       " 'angekommen ist',\n",
       " 'angelogen wird',\n",
       " 'angerufen um',\n",
       " 'angeschaut die',\n",
       " 'angesehen gute',\n",
       " 'angesprochen da',\n",
       " 'angesprochen werde',\n",
       " 'angestrengt pure',\n",
       " 'angetatscht zu',\n",
       " 'angetwerkt vielleicht',\n",
       " 'angewidert geschaut',\n",
       " 'angezeigt feedback',\n",
       " 'angezeigt werden',\n",
       " 'angibt dass',\n",
       " 'angkor wat',\n",
       " 'angrabschen mir',\n",
       " 'angst dass',\n",
       " 'angst jetzt',\n",
       " 'angst sie',\n",
       " 'angst von',\n",
       " 'angst vor',\n",
       " 'angst wieder',\n",
       " 'angucken und',\n",
       " 'ankommt verliebt',\n",
       " 'ankunft erst',\n",
       " 'anmache kommen',\n",
       " 'anmacht mein',\n",
       " 'annehmen da',\n",
       " 'annehmen egal',\n",
       " 'annika spam',\n",
       " 'annimmt danke',\n",
       " 'anprobieren einer',\n",
       " 'anrufe annehmen',\n",
       " 'ans bett',\n",
       " 'ans umziehen',\n",
       " 'anschauen könnte',\n",
       " 'anschauen so',\n",
       " 'ansehen müssen',\n",
       " 'ansichten halten',\n",
       " 'anspannen und',\n",
       " 'ansprechen hab',\n",
       " 'anspricht dich',\n",
       " 'anspuckst dann',\n",
       " 'anstrengend jeden',\n",
       " 'antenne verbogen',\n",
       " 'antibiotika trotz',\n",
       " 'antreibt bin',\n",
       " 'antun wie',\n",
       " 'antwort aber',\n",
       " 'antwort auf',\n",
       " 'antwort mehr',\n",
       " 'antwort weiß',\n",
       " 'antworten mit',\n",
       " 'antworten was',\n",
       " 'antworten würde',\n",
       " 'antwortet ihm',\n",
       " 'antwortet manchmal',\n",
       " 'anxiety keine',\n",
       " 'anzahl der',\n",
       " 'anzeige am',\n",
       " 'anzuhören bis',\n",
       " 'aok barmer',\n",
       " 'apache zweihundertsieben',\n",
       " 'apasche hat',\n",
       " 'app dabei',\n",
       " 'app einlösbar',\n",
       " 'app spinnt',\n",
       " 'appetit mehr',\n",
       " 'arbeit als',\n",
       " 'arbeit bzw',\n",
       " 'arbeit da',\n",
       " 'arbeit nichts',\n",
       " 'arbeit von',\n",
       " 'arbeite für',\n",
       " 'arbeiten ab',\n",
       " 'arbeiten habe',\n",
       " 'arbeiten müssen',\n",
       " 'arbeiten würde',\n",
       " 'arbeitet jemand',\n",
       " 'arbeitsplatz und',\n",
       " 'arbeitsrecht aus',\n",
       " 'arbeitstunden habe',\n",
       " 'arm nehmen',\n",
       " 'arm und',\n",
       " 'arme dom',\n",
       " 'armen umweltsäue',\n",
       " 'armesdeutschland dankemerkel',\n",
       " 'armut mich',\n",
       " 'arne friedrich',\n",
       " 'art seminar',\n",
       " 'artige durchsage',\n",
       " 'arzt auf',\n",
       " 'arzt dass',\n",
       " 'arzt der',\n",
       " 'arzt eine',\n",
       " 'arzt ich',\n",
       " 'arzt kennt',\n",
       " 'arzt verschrieben',\n",
       " 'arzt zu',\n",
       " 'as fuck',\n",
       " 'aschaffenburg müsste',\n",
       " 'assessment center',\n",
       " 'at me',\n",
       " 'at my',\n",
       " 'atlantotec und',\n",
       " 'atom kraftwerke',\n",
       " 'atomkraftwerken aus',\n",
       " 'attraktiven kellner',\n",
       " 'atzen auf',\n",
       " 'auch achtzehn',\n",
       " 'auch betroffene',\n",
       " 'auch dass',\n",
       " 'auch die',\n",
       " 'auch dumm',\n",
       " 'auch ein',\n",
       " 'auch eine',\n",
       " 'auch einen',\n",
       " 'auch einfach',\n",
       " 'auch endgültig',\n",
       " 'auch etwas',\n",
       " 'auch events',\n",
       " 'auch funktioniert',\n",
       " 'auch für',\n",
       " 'auch gefunden',\n",
       " 'auch gerne',\n",
       " 'auch geschenkt',\n",
       " 'auch glüh',\n",
       " 'auch gut',\n",
       " 'auch immer',\n",
       " 'auch in',\n",
       " 'auch kein',\n",
       " 'auch keingrünfürwenighoffnung',\n",
       " 'auch kinder',\n",
       " 'auch mal',\n",
       " 'auch man',\n",
       " 'auch mit',\n",
       " 'auch morgen',\n",
       " 'auch motorrad',\n",
       " 'auch nicht',\n",
       " 'auch noch',\n",
       " 'auch nur',\n",
       " 'auch oft',\n",
       " 'auch pflegen',\n",
       " 'auch schon',\n",
       " 'auch so',\n",
       " 'auch sonst',\n",
       " 'auch vegane',\n",
       " 'auch wenn',\n",
       " 'auch wenns',\n",
       " 'auch will',\n",
       " 'auch zu',\n",
       " 'auch zufrieden',\n",
       " 'auf alkohol',\n",
       " 'auf amazon',\n",
       " 'auf augsburg',\n",
       " 'auf autismus',\n",
       " 'auf avatar',\n",
       " 'auf bdsm',\n",
       " 'auf campus',\n",
       " 'auf das',\n",
       " 'auf dauer',\n",
       " 'auf dem',\n",
       " 'auf den',\n",
       " 'auf der',\n",
       " 'auf deutsch',\n",
       " 'auf die',\n",
       " 'auf diesen',\n",
       " 'auf ed',\n",
       " 'auf ein',\n",
       " 'auf eine',\n",
       " 'auf einem',\n",
       " 'auf einen',\n",
       " 'auf einer',\n",
       " 'auf einmal',\n",
       " 'auf ernsthafte',\n",
       " 'auf essen',\n",
       " 'auf euren',\n",
       " 'auf für',\n",
       " 'auf gar',\n",
       " 'auf gevoted',\n",
       " 'auf ghz',\n",
       " 'auf hat',\n",
       " 'auf insta',\n",
       " 'auf instagram',\n",
       " 'auf jodel',\n",
       " 'auf kiffen',\n",
       " 'auf machen',\n",
       " 'auf mein',\n",
       " 'auf meine',\n",
       " 'auf mich',\n",
       " 'auf mmf',\n",
       " 'auf mtv',\n",
       " 'auf muskatnuss',\n",
       " 'auf nazi',\n",
       " 'auf nein',\n",
       " 'auf ner',\n",
       " 'auf netflix',\n",
       " 'auf oldies',\n",
       " 'auf prime',\n",
       " 'auf rummachen',\n",
       " 'auf samstag',\n",
       " 'auf sein',\n",
       " 'auf sex',\n",
       " 'auf so',\n",
       " 'auf spiegel',\n",
       " 'auf spotify',\n",
       " 'auf südländische',\n",
       " 'auf telegram',\n",
       " 'auf tinder',\n",
       " 'auf unterdrückteaggressionenkommenheutealleraus',\n",
       " 'auf weihnachten',\n",
       " 'auf wochenenden',\n",
       " 'auf youtube',\n",
       " 'auf yt',\n",
       " 'auf über',\n",
       " 'aufeinander hocken',\n",
       " 'aufgebaut ist',\n",
       " 'aufgefallen das',\n",
       " 'aufgefallen dass',\n",
       " 'aufgefallen wie',\n",
       " 'aufgeklärt wurde',\n",
       " 'aufgemacht alter',\n",
       " 'aufgenommen haben',\n",
       " 'aufgenommen um',\n",
       " 'aufgeraut und',\n",
       " 'aufgeregte gefühl',\n",
       " 'aufgewachsen und',\n",
       " 'aufgewacht ich',\n",
       " 'aufgewacht todmüde',\n",
       " 'aufgewacht woher',\n",
       " 'aufgezogen haben',\n",
       " 'aufhören an',\n",
       " 'aufm alexanderplatz',\n",
       " 'aufm schiebeparkplatz',\n",
       " 'aufm weihnachtsmarkt',\n",
       " 'aufmacht gehen',\n",
       " 'aufnacht und',\n",
       " 'aufploppenden kronkorken',\n",
       " 'aufraffen wenn',\n",
       " 'aufregen fahre',\n",
       " 'aufrichtige zuneigung',\n",
       " 'aufs datum',\n",
       " 'aufs grundstück',\n",
       " 'aufs handy',\n",
       " 'aufs herz',\n",
       " 'aufstehen und',\n",
       " 'auftreiben könnte',\n",
       " 'auftritt mit',\n",
       " 'aufwachen fehlt',\n",
       " 'aufwand für',\n",
       " 'aufzuhören bin',\n",
       " 'augenbraue istkeinpickel',\n",
       " 'augenbrauenhaare sind',\n",
       " 'augsburg reagiert',\n",
       " 'aus aber',\n",
       " 'aus alle',\n",
       " 'aus allem',\n",
       " 'aus als',\n",
       " 'aus anderen',\n",
       " 'aus bin',\n",
       " 'aus ddm',\n",
       " 'aus dem',\n",
       " 'aus den',\n",
       " 'aus der',\n",
       " 'aus diesen',\n",
       " 'aus eifersucht',\n",
       " 'aus einer',\n",
       " 'aus für',\n",
       " 'aus gerade',\n",
       " 'aus herr',\n",
       " 'aus ich',\n",
       " 'aus irgendeinem',\n",
       " 'aus karlsruhe',\n",
       " 'aus meinem',\n",
       " 'aus meiner',\n",
       " 'aus melden',\n",
       " 'aus oder',\n",
       " 'aus raucht',\n",
       " 'aus rückschlägen',\n",
       " 'aus rücksicht',\n",
       " 'aus scham',\n",
       " 'aus versehen',\n",
       " 'ausarbeitung vom',\n",
       " 'ausbildung kann',\n",
       " 'ausdrucksweise hab',\n",
       " 'ausflug zum',\n",
       " 'ausfluss bekommen',\n",
       " 'ausgeben kann',\n",
       " 'ausgefallenere locations',\n",
       " 'ausgegangen wir',\n",
       " 'ausgehen kennt',\n",
       " 'ausgestiegen ist',\n",
       " 'auskennen oder',\n",
       " 'auskennt eine',\n",
       " 'ausland zu',\n",
       " 'auslandspraktikum einen',\n",
       " 'ausm kopf',\n",
       " 'ausm letzten',\n",
       " 'ausmaße die',\n",
       " 'ausprobiert die',\n",
       " 'ausprobiert und',\n",
       " 'ausschließlich synthesizer',\n",
       " 'aussehen mag',\n",
       " 'ausser griechisch',\n",
       " 'aussiehst wie',\n",
       " 'aussieht schmeckt',\n",
       " 'ausspuckt ich',\n",
       " 'aussteigen muss',\n",
       " 'austausch wäre',\n",
       " 'austauschen können',\n",
       " 'austauscht dann',\n",
       " 'auswahl an',\n",
       " 'auswahl könnt',\n",
       " 'ausweiche und',\n",
       " 'auszutauschen hat',\n",
       " 'autismus festgestellt',\n",
       " 'auto anspuckst',\n",
       " 'auto früher',\n",
       " 'auto meiner',\n",
       " 'auto mit',\n",
       " 'auto nicht',\n",
       " 'auto sauber',\n",
       " 'autobahn nach',\n",
       " 'autofahrer wenn',\n",
       " 'automatisch geblockt',\n",
       " 'autoplay bei',\n",
       " 'außen breiter',\n",
       " 'außer bei',\n",
       " 'außergewöhnlicher küche',\n",
       " 'avatar ist',\n",
       " 'avatar steht',\n",
       " 'avril lavigne',\n",
       " 'axl heck',\n",
       " 'ba also',\n",
       " 'baby du',\n",
       " 'baby neffe',\n",
       " 'babyface und',\n",
       " 'bachelorarbeit binden',\n",
       " 'bad urach',\n",
       " 'badtrip auf',\n",
       " 'bafög auch',\n",
       " 'baguette wie',\n",
       " 'bahn gearbeitet',\n",
       " 'bahn heroin',\n",
       " 'bahn mvgnervt',\n",
       " 'bahn steht',\n",
       " 'bahn von',\n",
       " 'bahn zwischen',\n",
       " 'bahncard die',\n",
       " 'bahnen ziehen',\n",
       " 'bahnhof meine',\n",
       " 'bahnhof mit',\n",
       " 'bahnsysteme des',\n",
       " 'bahnticket für',\n",
       " 'bakterienkultur und',\n",
       " 'bald ein',\n",
       " 'bald in',\n",
       " 'bald lehrer',\n",
       " 'bald mit',\n",
       " 'bald zwanzig',\n",
       " 'balkan oder',\n",
       " 'banane mit',\n",
       " 'bandelt bei',\n",
       " 'bands die',\n",
       " 'bangkok nach',\n",
       " 'bank war',\n",
       " 'bankangestellten am',\n",
       " 'bankkontos kennt',\n",
       " 'bannen moderatoren',\n",
       " 'bar meinen',\n",
       " 'barkeeper im',\n",
       " 'barmer oder',\n",
       " 'bars museen',\n",
       " 'bart schaut',\n",
       " 'basteln und',\n",
       " 'bauch op',\n",
       " 'baumfällung gemacht',\n",
       " 'bausa gesehen',\n",
       " 'bayern in',\n",
       " 'bayernpest verlieren',\n",
       " 'bedient worden',\n",
       " 'bedingt reimen',\n",
       " 'beef zu',\n",
       " 'beelitz heilstätten',\n",
       " 'beende das',\n",
       " 'beenden annehmen',\n",
       " 'beendet die',\n",
       " 'beendet und',\n",
       " 'beethoven böhmermann',\n",
       " 'befriedige fühle',\n",
       " 'befummeln lassen',\n",
       " 'begehrt help',\n",
       " 'beginnt der',\n",
       " 'beginnt meine',\n",
       " 'begründung war',\n",
       " 'begrüßung einfach',\n",
       " 'behutsam meinen',\n",
       " 'bei 100',\n",
       " 'bei 95',\n",
       " 'bei abgabe',\n",
       " 'bei aldi',\n",
       " 'bei allen',\n",
       " 'bei amazon',\n",
       " 'bei bayern',\n",
       " 'bei brands4friends',\n",
       " 'bei dem',\n",
       " 'bei den',\n",
       " 'bei denen',\n",
       " 'bei der',\n",
       " 'bei dir',\n",
       " 'bei edeka',\n",
       " 'bei einem',\n",
       " 'bei einer',\n",
       " 'bei garching',\n",
       " 'bei instagram',\n",
       " 'bei jeder',\n",
       " 'bei jodel',\n",
       " 'bei kik',\n",
       " 'bei kontroversen',\n",
       " 'bei mathe',\n",
       " 'bei mcdonalds',\n",
       " 'bei mir',\n",
       " 'bei mit',\n",
       " 'bei prof',\n",
       " 'bei sechs',\n",
       " 'bei seinem',\n",
       " 'bei spotify',\n",
       " 'bei subway',\n",
       " 'bei tinder',\n",
       " 'bei uns',\n",
       " 'bei wem',\n",
       " 'bei youtube',\n",
       " 'bei zu',\n",
       " 'beide auch',\n",
       " 'beide schlüsselbeine',\n",
       " 'beide stöhnen',\n",
       " 'beiden beim',\n",
       " 'beides so',\n",
       " 'beidseitig drucken',\n",
       " 'beigebracht haben',\n",
       " 'beim anblick',\n",
       " 'beim bj',\n",
       " 'beim bäcker',\n",
       " 'beim date',\n",
       " 'beim downvoten',\n",
       " 'beim gehen',\n",
       " 'beim iphone',\n",
       " 'beim lüften',\n",
       " 'beim mal',\n",
       " 'beim pinkeln',\n",
       " 'beim psychotherapeuten',\n",
       " 'beim satzger',\n",
       " 'beim sex',\n",
       " 'beim shoppen',\n",
       " 'beim sport',\n",
       " 'beim tübinger',\n",
       " 'beim up',\n",
       " 'beim vögeln',\n",
       " 'bein gerieben',\n",
       " 'bej dir',\n",
       " 'bekannt dass',\n",
       " 'bekannte von',\n",
       " 'bekannten im',\n",
       " 'bekanntes sein',\n",
       " 'bekloppten namen',\n",
       " 'bekomm es',\n",
       " 'bekomme gleich',\n",
       " 'bekomme hier',\n",
       " 'bekomme ich',\n",
       " 'bekomme verdammtefeiertage',\n",
       " 'bekommen aber',\n",
       " 'bekommen bestesgeschenk',\n",
       " 'bekommen das',\n",
       " 'bekommen dass',\n",
       " 'bekommen habe',\n",
       " 'bekommen hat',\n",
       " 'bekommen ich',\n",
       " 'bekommen obwohl',\n",
       " 'bekommen und',\n",
       " 'bekommen was',\n",
       " 'bekommen wir',\n",
       " 'bekommen zudritt',\n",
       " 'bekommst tagversüßt',\n",
       " 'bekommt aber',\n",
       " 'bekommt damit',\n",
       " 'bekommt machen',\n",
       " 'bekommt man',\n",
       " 'bekommt und',\n",
       " 'belagert hat',\n",
       " 'belangt werden',\n",
       " 'belehrt dass',\n",
       " 'beleidigt und',\n",
       " 'belästigen butwhynot',\n",
       " 'belästigt dass',\n",
       " 'ben and',\n",
       " 'benachrichtungston einen',\n",
       " 'benandjerrys foodlove',\n",
       " 'benutz doch',\n",
       " 'benutze die',\n",
       " 'benutzen hatte',\n",
       " 'benutzen lassen',\n",
       " 'benzin und',\n",
       " 'bereich und',\n",
       " 'bereit für',\n",
       " 'bereitet jetzt',\n",
       " 'bereits mittels',\n",
       " 'bereut an',\n",
       " 'bergauf und',\n",
       " 'bericht im',\n",
       " 'berlin einen',\n",
       " 'berlin geboren',\n",
       " 'berlin gibst',\n",
       " 'berlin ist',\n",
       " 'berlin just',\n",
       " 'berlin und',\n",
       " 'berliner weil',\n",
       " 'beruf besucht',\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame (unigram_feature_vector, columns = uni_vectorizer.get_feature_names())\n",
    "bi_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram feature matrices\n",
    "\n",
    "#unigram\n",
    "#uni_vectorizer = CountVectorizer(stop_words=stopwords.words('german'), ngram_range=(1,1))\n",
    "#unigram_feature_vector = uni_vectorizer.fit_transform(jhj_emoji_texts).toarray() \n",
    "\n",
    "#uni and bigram\n",
    "#uni_bi_vectorizer = CountVectorizer(stop_words=stopwords.words('german'), ngram_range=(1,2))\n",
    "#unigram_bi_feature_vector = uni_bi_vectorizer.fit_transform(jhj_emoji_texts).toarray() \n",
    "\n",
    "#bigram\n",
    "#bi_vectorizer = CountVectorizer(stop_words=stopwords.words('german'), ngram_range=(2,2))\n",
    "#bigram_feature_vector = bi_vectorizer.fit_transform(jhj_emoji_texts).toarray()\n",
    "\n",
    "\n",
    "#trigram\n",
    "#tri_vectorizer = CountVectorizer(stop_words=stopwords.words('german'), ngram_range=(2,2))\n",
    "#trigram_feature_vector = tri_vectorizer.fit_transform(jhj_emoji_texts).toarray() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print data frame to see whether preprocessing changes the content, also look it up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf vectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('german'))\n",
    "tfidf = tfidf_vectorizer.fit_transform(jhj_emoji_texts).toarray() #feature matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf vectorizer ohne stopwords zu löschen\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform(jhj_emoji_texts).toarray() #feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "858\n"
     ]
    }
   ],
   "source": [
    "#numerical features\n",
    "#average length of post, average sentence length, average number of punctuation\n",
    "#punctuation weg lassen weil nicht wirklich aussagekräftig\n",
    "\n",
    "\n",
    "import numpy\n",
    "import string\n",
    "from nltk import tokenize\n",
    "\n",
    "post_length = [] #feature vector\n",
    "counter = 0\n",
    "number_of_punct = [] #feature vector\n",
    "sentence_lengths = [] \n",
    "tokenized_posts = []\n",
    "for text in jhj_emoji_texts:\n",
    "    post_length.append([len(text.split())])\n",
    "    tokenized_posts.append(tokenize.sent_tokenize(text))\n",
    "    for char in text:\n",
    "        if char in string.punctuation:\n",
    "            counter+=1\n",
    "    number_of_punct.append([counter])\n",
    "    counter = 0\n",
    "sent = []\n",
    "for post in tokenized_posts:\n",
    "    for sentence in post:\n",
    "        sent.append(len(sentence.split()))\n",
    "    sentence_lengths.append(sent)\n",
    "    sent = []\n",
    "#get average\n",
    "average_sent_len = [] #feature vector\n",
    "for post in sentence_lengths:\n",
    "    average_sent_len.append([sum(post)/len(post)])    \n",
    "print(len(sentence_lengths)) #why 2169?? need:1269 need one number, maybe the average sentence length?\n",
    "#print(average_sent_len)\n",
    "#print(sentence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "858\n"
     ]
    }
   ],
   "source": [
    "#POS features\n",
    "import spacy\n",
    "nlp = spacy.load('de_core_news_md')\n",
    "\n",
    "#get POStags via spacy\n",
    "spacy_obj = []\n",
    "post_as_pos_tags = []\n",
    "as_pos = [] #this\n",
    "for text in jhj_emoji_texts:\n",
    "    spacy_obj.append(nlp(text))\n",
    "for text in spacy_obj:\n",
    "    for token in text:\n",
    "        post_as_pos_tags.append(token.tag_)\n",
    "    as_pos.append(' '.join(post_as_pos_tags))\n",
    "    post_as_pos_tags = []\n",
    "\n",
    "#neue POS-features\n",
    "ADV_count = []\n",
    "PPER_count = []\n",
    "comma_count = []\n",
    "NE_count = []\n",
    "NN_count = []\n",
    "ADJD_count = []\n",
    "for post in as_pos:\n",
    "    ADV_count.append([post.count(\"ADV\")])\n",
    "    PPER_count.append([post.count(\"PPER\")])\n",
    "    comma_count.append([post.count(\"$,\")])\n",
    "    NE_count.append([post.count(\"NE\")])\n",
    "    NN_count.append([post.count(\"NN\")])\n",
    "    ADJD_count.append([post.count(\"ADJD\")])\n",
    "    \n",
    "print(len(ADJD_count))\n",
    "\n",
    "#get posts as sequence of pos tags\n",
    "#as_pos = sequence of pos tags\n",
    "#make sequences into count vectorizers\n",
    "pos_vec_uni = CountVectorizer(ngram_range=(1,1))\n",
    "pos_uni_count_vec = pos_vec_uni.fit_transform(as_pos).toarray() #uni feature vector\n",
    "pos_vec_bi = CountVectorizer(ngram_range=(2,2))\n",
    "pos_bi_count_vec = pos_vec_bi.fit_transform(as_pos).toarray() #bi feature vector\n",
    "#make theminto tfidf vectorizers\n",
    "tfidf_vectorizer_pos = TfidfVectorizer()\n",
    "tfidf_pos = tfidf_vectorizer_pos.fit_transform(as_pos) #tfidf pos feature matrix hat das iwelche benefits??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try wordvectors instead of ngrams as feature?\n",
    "\n",
    "spacy_posts = []\n",
    "vector_posts = []\n",
    "temp_post = []\n",
    "for post in jhj_emoji_texts:\n",
    "    spacy_posts.append(nlp(post))\n",
    "for post in spacy_posts:\n",
    "    for word in post:\n",
    "        temp_post.append(word.vector)\n",
    "    vector_posts.append(temp_post)\n",
    "    temp_post = []\n",
    "    \n",
    "mean_vector_posts = []\n",
    "for post_list in vector_posts:\n",
    "    #print(len(post))\n",
    "    mean_vector_posts.append([sum(post_list)/len(post_list)])\n",
    "    \n",
    "word_vector_feature = numpy.concatenate(mean_vector_posts, axis=0)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################combine feature-vectors:##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine feature-vectors:\n",
    "\n",
    "#uni-grams\n",
    "#unigram_feature_vector\n",
    "\n",
    "#bigrams\n",
    "#bigram_feature_vector\n",
    "\n",
    "#trigrams\n",
    "#trigram_feature_vector\n",
    "\n",
    "#unigrams and bigrams\n",
    "#unigram_bigram_feature_vector\n",
    "\n",
    "#uni-grams, bigrams, trigrams\n",
    "uni_bi_tri_feat_vec = numpy.append(unigram_feature_vector, bigram_feature_vector, 1)\n",
    "uni_bi_tri_feat_vec = numpy.append(uni_bi_tri_feat_vec,trigram_feature_vector, 1)\n",
    "\n",
    "\n",
    "#unigrams, trigrams\n",
    "uni_tri_feat_vec = numpy.append(unigram_feature_vector, trigram_feature_vector, 1)\n",
    "#bigrams, trigams\n",
    "bi_tri_feat_vec = numpy.append(bigram_feature_vector, trigram_feature_vector, 1)\n",
    "\n",
    "\n",
    "\n",
    "#neue POS-features\n",
    "pos = numpy.append(ADV_count, PPER_count, 1)\n",
    "pos = numpy.append(pos, comma_count, 1)\n",
    "pos = numpy.append(pos, NE_count, 1)\n",
    "pos = numpy.append(pos, NN_count, 1)\n",
    "pos = numpy.append(pos, ADJD_count, 1)\n",
    "\n",
    "\n",
    "#ngrams and POS features\n",
    "uni_pos_new = numpy.append(unigram_feature_vector, pos, 1)\n",
    "bi_new_pos = numpy.append(bigram_feature_vector, pos, 1)\n",
    "tri_new_pos = numpy.append(trigram_feature_vector, pos, 1)\n",
    "\n",
    "#numerical features\n",
    "#sentence_length\n",
    "post_sent = numpy.append(post_length, average_sent_len, 1)\n",
    "post_sent_punct = numpy.append(post_sent, number_of_punct, 1)\n",
    "\n",
    "#POS and numerical features\n",
    "pos_post_sent_len = numpy.append(pos, post_sent, 1)\n",
    "pos_num = numpy.append(post_sent_punct, pos, 1)\n",
    "\n",
    "\n",
    "#unigrams with numerical features\n",
    "uni_postlegth_feat_vec = numpy.append(unigram_feature_vector, post_length, 1)\n",
    "uni_punct = numpy.append(unigram_feature_vector, number_of_punct, 1)\n",
    "uni_sent_len = numpy.append(unigram_feature_vector, average_sent_len, 1)\n",
    "uni_post_sent_len = numpy.append(unigram_feature_vector, post_sent, 1)\n",
    "uni_num = numpy.append(unigram_feature_vector, post_sent_punct, 1)\n",
    "\n",
    "#bigrams with numerical features\n",
    "bi_post = numpy.append(bigram_feature_vector, post_length, 1)\n",
    "bi_sent = numpy.append(bigram_feature_vector, average_sent_len, 1)\n",
    "bi_punct = numpy.append(bigram_feature_vector, number_of_punct, 1)\n",
    "bi_sent_post = numpy.append(bigram_feature_vector, post_sent, 1)\n",
    "bi_num = numpy.append(bigram_feature_vector, post_sent_punct, 1)\n",
    "\n",
    "#trigrams with numerical features\n",
    "tri_post = numpy.append(trigram_feature_vector, post_length, 1)\n",
    "tri_sent = numpy.append(trigram_feature_vector, average_sent_len, 1)\n",
    "tri_punct = numpy.append(trigram_feature_vector, number_of_punct, 1)\n",
    "tri_sent_post = numpy.append(trigram_feature_vector, post_sent, 1)\n",
    "tri_num = numpy.append(trigram_feature_vector, post_sent_punct, 1)\n",
    "\n",
    "\n",
    "#unigrams, bigrams, numerical features\n",
    "uni_bi_post = numpy.append(unigram_bi_feature_vector, post_length, 1)\n",
    "uni_bi_sent = numpy.append(unigram_bi_feature_vector, average_sent_len, 1)\n",
    "uni_bi_punct = numpy.append(unigram_bi_feature_vector, number_of_punct, 1)\n",
    "uni_bi_sent_post = numpy.append(unigram_bi_feature_vector, post_sent, 1)\n",
    "uni_bi_num = numpy.append(unigram_bi_feature_vector, post_sent_punct, 1)\n",
    "\n",
    "#ngrams with POS and numericals\n",
    "uni_post_sent_pos = numpy.append(uni_post_sent_len, pos, 1)\n",
    "bi_post_sent_pos = numpy.append(bi_sent_post, pos, 1)\n",
    "tri_post_sent_pos = numpy.append(tri_sent_post, pos, 1)\n",
    "\n",
    "\n",
    "#wordvectors : word_vector_feature\n",
    "\n",
    "#wordvectors and ngrams\n",
    "uni_wordvector = numpy.append(unigram_feature_vector, word_vector_feature, 1)\n",
    "bi_wordvector = numpy.append(bigram_feature_vector, word_vector_feature, 1)\n",
    "tri_wordvector = numpy.append(trigram_feature_vector, word_vector_feature, 1)\n",
    "\n",
    "#wordvectors and numerical features\n",
    "wordvector_post = numpy.append(word_vector_feature, post_length, 1)\n",
    "wordvector_sent = numpy.append(word_vector_feature, average_sent_len, 1)\n",
    "wordvector_post_sent = numpy.append(word_vector_feature, post_sent, 1)\n",
    "wordvector_post_sent_punct = numpy.append(word_vector_feature, post_sent_punct, 1)\n",
    "\n",
    "#wordvectors and POS features\n",
    "wordvector_pos = numpy.append(word_vector_feature, pos, 1)\n",
    "#wordvectors and POS features and numericals\n",
    "wordvector_pos_num =  numpy.append(wordvector_post_sent , pos, 1)\n",
    "wordvector_pos_num =  numpy.append(wordvector_pos_num , number_of_punct, 1)\n",
    "\n",
    "#wordvectors and tfidf\n",
    "wordvector_tfidf = numpy.append(word_vector_feature, tfidf, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################logistic regression##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 28\n",
      "[[ 24  14]\n",
      " [ 14 120]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.63      0.63        38\n",
      "           1       0.90      0.90      0.90       134\n",
      "\n",
      "    accuracy                           0.84       172\n",
      "   macro avg       0.76      0.76      0.76       172\n",
      "weighted avg       0.84      0.84      0.84       172\n",
      "\n",
      "accuracy score:  0.8372093023255814\n"
     ]
    }
   ],
   "source": [
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = tfidf #data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced')\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00 00</th>\n",
       "      <th>00 uhr</th>\n",
       "      <th>00 was</th>\n",
       "      <th>000 zahle</th>\n",
       "      <th>08 15</th>\n",
       "      <th>10 10</th>\n",
       "      <th>10 daddy</th>\n",
       "      <th>10 flixbus</th>\n",
       "      <th>10 geschenk</th>\n",
       "      <th>10 jahren</th>\n",
       "      <th>...</th>\n",
       "      <th>überrascht von</th>\n",
       "      <th>überrascht wie</th>\n",
       "      <th>übers wochenende</th>\n",
       "      <th>überspringen sich</th>\n",
       "      <th>übertrieben lust</th>\n",
       "      <th>überwiesen bekommen</th>\n",
       "      <th>überwindung gekostet</th>\n",
       "      <th>üblichen hitler</th>\n",
       "      <th>übrig uns</th>\n",
       "      <th>übrigens auch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>858 rows × 11555 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     00 00  00 uhr  00 was  000 zahle  08 15  10 10  10 daddy  10 flixbus  \\\n",
       "0        0       0       0          0      0      0         0           0   \n",
       "1        0       0       0          0      0      0         0           0   \n",
       "2        0       0       0          0      0      0         0           0   \n",
       "3        0       0       0          0      0      0         0           0   \n",
       "4        0       0       0          0      0      0         0           0   \n",
       "..     ...     ...     ...        ...    ...    ...       ...         ...   \n",
       "853      0       0       0          0      0      0         0           0   \n",
       "854      0       0       0          0      0      0         0           0   \n",
       "855      0       0       0          0      0      0         0           0   \n",
       "856      0       0       0          0      0      0         0           0   \n",
       "857      0       0       0          0      0      0         0           0   \n",
       "\n",
       "     10 geschenk  10 jahren  ...  überrascht von  überrascht wie  \\\n",
       "0              0          0  ...               0               0   \n",
       "1              0          0  ...               0               0   \n",
       "2              0          0  ...               0               0   \n",
       "3              0          0  ...               0               0   \n",
       "4              0          0  ...               0               0   \n",
       "..           ...        ...  ...             ...             ...   \n",
       "853            0          0  ...               0               0   \n",
       "854            0          0  ...               0               0   \n",
       "855            0          0  ...               0               0   \n",
       "856            0          0  ...               0               0   \n",
       "857            0          0  ...               0               0   \n",
       "\n",
       "     übers wochenende  überspringen sich  übertrieben lust  \\\n",
       "0                   0                  0                 0   \n",
       "1                   0                  0                 0   \n",
       "2                   0                  0                 0   \n",
       "3                   0                  0                 0   \n",
       "4                   0                  0                 0   \n",
       "..                ...                ...               ...   \n",
       "853                 0                  0                 0   \n",
       "854                 0                  0                 0   \n",
       "855                 0                  0                 0   \n",
       "856                 0                  0                 0   \n",
       "857                 0                  0                 0   \n",
       "\n",
       "     überwiesen bekommen  überwindung gekostet  üblichen hitler  übrig uns  \\\n",
       "0                      0                     0                0          0   \n",
       "1                      0                     0                0          0   \n",
       "2                      0                     0                0          0   \n",
       "3                      0                     0                0          0   \n",
       "4                      0                     0                0          0   \n",
       "..                   ...                   ...              ...        ...   \n",
       "853                    0                     0                0          0   \n",
       "854                    0                     0                0          0   \n",
       "855                    0                     0                0          0   \n",
       "856                    0                     0                0          0   \n",
       "857                    0                     0                0          0   \n",
       "\n",
       "     übrigens auch  \n",
       "0                0  \n",
       "1                0  \n",
       "2                0  \n",
       "3                0  \n",
       "4                0  \n",
       "..             ...  \n",
       "853              0  \n",
       "854              0  \n",
       "855              0  \n",
       "856              0  \n",
       "857              0  \n",
       "\n",
       "[858 rows x 11555 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3880 sind die verschiedenen worte\n",
    "import pandas as pd\n",
    "pd.DataFrame(bigram_feature_vector, columns=bi_vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature:  gerade  score:  0.8101654839280588\n",
      "feature:  jetzt  score:  1.223768602748592\n",
      "feature:  lust  score:  1.0144876679144346\n",
      "feature:  meinen  score:  0.9170042281433811\n",
      "feature:  nur  score:  0.9238682703862068\n",
      "feature:  sein  score:  0.9849367934635365\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# logistic regression for feature importance\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "X = unigram_feature_vector #data\n",
    "y = jhj_emoji_labels#target\n",
    "# define the model\n",
    "model = LogisticRegression()\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# get importance\n",
    "importance = model.coef_[0]\n",
    "for score,name in zip(importance, uni_vectorizer.get_feature_names()):\n",
    "    if score > 0.80:\n",
    "        print(\"feature: \", name, \" score: \", score)\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# logistic regression for feature importance\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "X = bigram_feature_vector #data\n",
    "y = jhj_emoji_labels#target\n",
    "# define the model\n",
    "model = LogisticRegression()\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# get importance\n",
    "importance = model.coef_[0]\n",
    "for score,name in zip(importance, uni_vectorizer.get_feature_names()):\n",
    "    if score > 0.70:\n",
    "        print(\"feature: \", name, \" score: \", score)\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: 0.08778\n",
      "Feature: 1, Score: 0.07191\n",
      "Feature: 2, Score: 0.09141\n",
      "Feature: 3, Score: 0.06385\n",
      "Feature: 4, Score: -0.08275\n",
      "Feature: 5, Score: 0.28912\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# logistic regression for feature importance\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "X = pos #data\n",
    "y = jhj_emoji_labels#target\n",
    "# define the model\n",
    "model = LogisticRegression()\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# get importance\n",
    "importance = model.coef_[0]\n",
    "#for score,name in zip(importance, uni_vectorizer.get_feature_names()):\n",
    " #   if score < -0.6:\n",
    "        #print(\"feature: \", name, \" score: \", score)\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: -0.00832\n",
      "Feature: 1, Score: 0.07751\n",
      "Feature: 2, Score: -0.05423\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# logistic regression for feature importance\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "X = post_sent_punct #data\n",
    "y = jhj_emoji_labels#target\n",
    "# define the model\n",
    "model = LogisticRegression()\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# get importance\n",
    "importance = model.coef_[0]\n",
    "#for score,name in zip(importance, uni_vectorizer.get_feature_names()):\n",
    " #   if score < -0.6:\n",
    "        #print(\"feature: \", name, \" score: \", score)\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()\n",
    "\n",
    "#post_sent = numpy.append(post_length, average_sent_len, 1)\n",
    "#post_sent_punct = numpy.append(post_sent, number_of_punct, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: 0.45703\n",
      "Feature: 1, Score: 0.79132\n",
      "Feature: 2, Score: 0.04121\n",
      "Feature: 3, Score: -0.34565\n",
      "Feature: 4, Score: 0.07490\n",
      "Feature: 5, Score: 0.08345\n",
      "Feature: 6, Score: -0.48703\n",
      "Feature: 7, Score: -0.01140\n",
      "Feature: 8, Score: 0.17524\n",
      "Feature: 9, Score: -0.06040\n",
      "Feature: 10, Score: -0.10564\n",
      "Feature: 11, Score: -0.28655\n",
      "Feature: 12, Score: -0.64862\n",
      "Feature: 13, Score: -0.38420\n",
      "Feature: 14, Score: 0.15991\n",
      "Feature: 15, Score: -0.48304\n",
      "Feature: 16, Score: 0.62602\n",
      "Feature: 17, Score: -0.21458\n",
      "Feature: 18, Score: 0.57268\n",
      "Feature: 19, Score: -0.04151\n",
      "Feature: 20, Score: 0.72740\n",
      "Feature: 21, Score: 1.09670\n",
      "Feature: 22, Score: -0.25967\n",
      "Feature: 23, Score: 0.40695\n",
      "Feature: 24, Score: -0.15929\n",
      "Feature: 25, Score: 0.08047\n",
      "Feature: 26, Score: 0.04740\n",
      "Feature: 27, Score: 0.42538\n",
      "Feature: 28, Score: -0.49946\n",
      "Feature: 29, Score: 0.56763\n",
      "Feature: 30, Score: -0.20690\n",
      "Feature: 31, Score: 0.17904\n",
      "Feature: 32, Score: -0.12729\n",
      "Feature: 33, Score: -0.15054\n",
      "Feature: 34, Score: 0.12183\n",
      "Feature: 35, Score: -1.01442\n",
      "Feature: 36, Score: -0.29417\n",
      "Feature: 37, Score: -0.49944\n",
      "Feature: 38, Score: -0.31052\n",
      "Feature: 39, Score: -0.28361\n",
      "Feature: 40, Score: 1.03371\n",
      "Feature: 41, Score: 0.15229\n",
      "Feature: 42, Score: -0.47646\n",
      "Feature: 43, Score: -0.75764\n",
      "Feature: 44, Score: -0.17695\n",
      "Feature: 45, Score: 0.79571\n",
      "Feature: 46, Score: -0.19625\n",
      "Feature: 47, Score: 0.22382\n",
      "Feature: 48, Score: 0.55013\n",
      "Feature: 49, Score: 0.40487\n",
      "Feature: 50, Score: 0.40034\n",
      "Feature: 51, Score: 0.03699\n",
      "Feature: 52, Score: 0.32500\n",
      "Feature: 53, Score: -0.18938\n",
      "Feature: 54, Score: -0.01654\n",
      "Feature: 55, Score: -0.61459\n",
      "Feature: 56, Score: -1.10966\n",
      "Feature: 57, Score: -0.00410\n",
      "Feature: 58, Score: -0.25604\n",
      "Feature: 59, Score: -0.12265\n",
      "Feature: 60, Score: 0.40920\n",
      "Feature: 61, Score: 0.70030\n",
      "Feature: 62, Score: -0.02147\n",
      "Feature: 63, Score: 0.57050\n",
      "Feature: 64, Score: 0.02928\n",
      "Feature: 65, Score: 0.18624\n",
      "Feature: 66, Score: -0.21289\n",
      "Feature: 67, Score: 0.11551\n",
      "Feature: 68, Score: -0.69028\n",
      "Feature: 69, Score: -0.04822\n",
      "Feature: 70, Score: 0.00409\n",
      "Feature: 71, Score: 0.02375\n",
      "Feature: 72, Score: -0.60561\n",
      "Feature: 73, Score: -0.49945\n",
      "Feature: 74, Score: -0.04381\n",
      "Feature: 75, Score: 0.06667\n",
      "Feature: 76, Score: -0.14551\n",
      "Feature: 77, Score: -0.21143\n",
      "Feature: 78, Score: -0.71903\n",
      "Feature: 79, Score: 0.09488\n",
      "Feature: 80, Score: -0.13831\n",
      "Feature: 81, Score: -0.94838\n",
      "Feature: 82, Score: -0.00533\n",
      "Feature: 83, Score: -0.44909\n",
      "Feature: 84, Score: -0.84037\n",
      "Feature: 85, Score: -0.25274\n",
      "Feature: 86, Score: 0.35888\n",
      "Feature: 87, Score: -0.36001\n",
      "Feature: 88, Score: -0.17089\n",
      "Feature: 89, Score: 0.23741\n",
      "Feature: 90, Score: -0.80690\n",
      "Feature: 91, Score: -0.28416\n",
      "Feature: 92, Score: 0.66579\n",
      "Feature: 93, Score: -0.83321\n",
      "Feature: 94, Score: -0.00059\n",
      "Feature: 95, Score: 0.58026\n",
      "Feature: 96, Score: -0.45430\n",
      "Feature: 97, Score: -0.60214\n",
      "Feature: 98, Score: 0.73169\n",
      "Feature: 99, Score: -0.00689\n",
      "Feature: 100, Score: 0.07764\n",
      "Feature: 101, Score: -0.63097\n",
      "Feature: 102, Score: 0.43126\n",
      "Feature: 103, Score: 0.30417\n",
      "Feature: 104, Score: 1.09810\n",
      "Feature: 105, Score: -0.13602\n",
      "Feature: 106, Score: -0.42544\n",
      "Feature: 107, Score: -0.81458\n",
      "Feature: 108, Score: 0.80714\n",
      "Feature: 109, Score: 0.43243\n",
      "Feature: 110, Score: 0.33604\n",
      "Feature: 111, Score: -0.46970\n",
      "Feature: 112, Score: -0.53364\n",
      "Feature: 113, Score: -0.06535\n",
      "Feature: 114, Score: 0.16343\n",
      "Feature: 115, Score: -0.26152\n",
      "Feature: 116, Score: -0.09693\n",
      "Feature: 117, Score: 0.19196\n",
      "Feature: 118, Score: 0.55388\n",
      "Feature: 119, Score: -0.25781\n",
      "Feature: 120, Score: -0.26491\n",
      "Feature: 121, Score: -0.21320\n",
      "Feature: 122, Score: 0.14283\n",
      "Feature: 123, Score: 0.13022\n",
      "Feature: 124, Score: 0.64709\n",
      "Feature: 125, Score: -0.37979\n",
      "Feature: 126, Score: -0.05470\n",
      "Feature: 127, Score: 0.39074\n",
      "Feature: 128, Score: -0.31540\n",
      "Feature: 129, Score: 0.56954\n",
      "Feature: 130, Score: -0.49556\n",
      "Feature: 131, Score: 0.17914\n",
      "Feature: 132, Score: 0.20349\n",
      "Feature: 133, Score: -0.12100\n",
      "Feature: 134, Score: -0.64271\n",
      "Feature: 135, Score: 0.89979\n",
      "Feature: 136, Score: -0.23839\n",
      "Feature: 137, Score: -0.10115\n",
      "Feature: 138, Score: 0.32066\n",
      "Feature: 139, Score: 0.62815\n",
      "Feature: 140, Score: 0.20767\n",
      "Feature: 141, Score: 0.73518\n",
      "Feature: 142, Score: 0.72179\n",
      "Feature: 143, Score: 0.28350\n",
      "Feature: 144, Score: -0.67860\n",
      "Feature: 145, Score: 0.12961\n",
      "Feature: 146, Score: -0.29282\n",
      "Feature: 147, Score: -0.08019\n",
      "Feature: 148, Score: 0.83966\n",
      "Feature: 149, Score: 0.39808\n",
      "Feature: 150, Score: 0.83452\n",
      "Feature: 151, Score: -0.00038\n",
      "Feature: 152, Score: -0.63962\n",
      "Feature: 153, Score: -0.71187\n",
      "Feature: 154, Score: 0.12608\n",
      "Feature: 155, Score: -0.41410\n",
      "Feature: 156, Score: -0.20819\n",
      "Feature: 157, Score: -0.01566\n",
      "Feature: 158, Score: 0.31995\n",
      "Feature: 159, Score: 0.95858\n",
      "Feature: 160, Score: 0.06082\n",
      "Feature: 161, Score: -0.33036\n",
      "Feature: 162, Score: 0.31420\n",
      "Feature: 163, Score: 0.10358\n",
      "Feature: 164, Score: 0.19339\n",
      "Feature: 165, Score: -0.67115\n",
      "Feature: 166, Score: -0.04110\n",
      "Feature: 167, Score: 0.94521\n",
      "Feature: 168, Score: -0.14014\n",
      "Feature: 169, Score: 0.41498\n",
      "Feature: 170, Score: -0.24161\n",
      "Feature: 171, Score: 0.06871\n",
      "Feature: 172, Score: -0.07860\n",
      "Feature: 173, Score: -0.30117\n",
      "Feature: 174, Score: -0.67030\n",
      "Feature: 175, Score: 0.57099\n",
      "Feature: 176, Score: -0.46284\n",
      "Feature: 177, Score: 0.10622\n",
      "Feature: 178, Score: -0.20423\n",
      "Feature: 179, Score: -0.45784\n",
      "Feature: 180, Score: 0.89166\n",
      "Feature: 181, Score: -0.56774\n",
      "Feature: 182, Score: 0.00175\n",
      "Feature: 183, Score: -0.41675\n",
      "Feature: 184, Score: 0.08737\n",
      "Feature: 185, Score: 0.02712\n",
      "Feature: 186, Score: -0.35961\n",
      "Feature: 187, Score: -0.21231\n",
      "Feature: 188, Score: -0.35541\n",
      "Feature: 189, Score: 0.40264\n",
      "Feature: 190, Score: -0.96088\n",
      "Feature: 191, Score: 0.14313\n",
      "Feature: 192, Score: 0.15469\n",
      "Feature: 193, Score: -0.14128\n",
      "Feature: 194, Score: 0.03460\n",
      "Feature: 195, Score: 0.30981\n",
      "Feature: 196, Score: 0.33560\n",
      "Feature: 197, Score: -0.34096\n",
      "Feature: 198, Score: -0.68594\n",
      "Feature: 199, Score: 0.07712\n",
      "Feature: 200, Score: -0.23861\n",
      "Feature: 201, Score: 0.02547\n",
      "Feature: 202, Score: 1.15799\n",
      "Feature: 203, Score: 0.33915\n",
      "Feature: 204, Score: -0.22691\n",
      "Feature: 205, Score: 0.65616\n",
      "Feature: 206, Score: -0.12350\n",
      "Feature: 207, Score: -0.02121\n",
      "Feature: 208, Score: -0.79174\n",
      "Feature: 209, Score: 0.37639\n",
      "Feature: 210, Score: -0.17139\n",
      "Feature: 211, Score: 0.39646\n",
      "Feature: 212, Score: 0.17923\n",
      "Feature: 213, Score: -0.25889\n",
      "Feature: 214, Score: 0.12186\n",
      "Feature: 215, Score: -0.08720\n",
      "Feature: 216, Score: 0.10851\n",
      "Feature: 217, Score: -0.65785\n",
      "Feature: 218, Score: 0.42952\n",
      "Feature: 219, Score: 0.30291\n",
      "Feature: 220, Score: -0.43091\n",
      "Feature: 221, Score: -0.10701\n",
      "Feature: 222, Score: 0.18848\n",
      "Feature: 223, Score: 0.29643\n",
      "Feature: 224, Score: 0.34260\n",
      "Feature: 225, Score: 0.12005\n",
      "Feature: 226, Score: 0.00989\n",
      "Feature: 227, Score: 0.49972\n",
      "Feature: 228, Score: 0.10093\n",
      "Feature: 229, Score: -0.15600\n",
      "Feature: 230, Score: 0.16534\n",
      "Feature: 231, Score: -0.30666\n",
      "Feature: 232, Score: -0.42256\n",
      "Feature: 233, Score: -0.18453\n",
      "Feature: 234, Score: 0.54381\n",
      "Feature: 235, Score: 0.46827\n",
      "Feature: 236, Score: -0.89316\n",
      "Feature: 237, Score: 0.33191\n",
      "Feature: 238, Score: -0.80778\n",
      "Feature: 239, Score: 0.56395\n",
      "Feature: 240, Score: 0.37836\n",
      "Feature: 241, Score: 0.05002\n",
      "Feature: 242, Score: -0.28513\n",
      "Feature: 243, Score: -0.04293\n",
      "Feature: 244, Score: -0.13248\n",
      "Feature: 245, Score: -0.63275\n",
      "Feature: 246, Score: -0.40349\n",
      "Feature: 247, Score: 0.22416\n",
      "Feature: 248, Score: -1.29835\n",
      "Feature: 249, Score: -0.11125\n",
      "Feature: 250, Score: -0.50716\n",
      "Feature: 251, Score: 0.31617\n",
      "Feature: 252, Score: 0.08678\n",
      "Feature: 253, Score: -0.02386\n",
      "Feature: 254, Score: -0.08298\n",
      "Feature: 255, Score: -0.54213\n",
      "Feature: 256, Score: -0.08202\n",
      "Feature: 257, Score: -0.64565\n",
      "Feature: 258, Score: 0.29240\n",
      "Feature: 259, Score: 0.38796\n",
      "Feature: 260, Score: -0.00776\n",
      "Feature: 261, Score: 0.75412\n",
      "Feature: 262, Score: -0.04645\n",
      "Feature: 263, Score: -0.21890\n",
      "Feature: 264, Score: 0.55352\n",
      "Feature: 265, Score: 0.21790\n",
      "Feature: 266, Score: -0.50768\n",
      "Feature: 267, Score: 0.59870\n",
      "Feature: 268, Score: 0.72933\n",
      "Feature: 269, Score: -0.73916\n",
      "Feature: 270, Score: 0.49768\n",
      "Feature: 271, Score: -0.88067\n",
      "Feature: 272, Score: 0.09096\n",
      "Feature: 273, Score: 0.15886\n",
      "Feature: 274, Score: 0.63619\n",
      "Feature: 275, Score: 0.19402\n",
      "Feature: 276, Score: -0.81345\n",
      "Feature: 277, Score: 0.26723\n",
      "Feature: 278, Score: -0.82121\n",
      "Feature: 279, Score: -0.19729\n",
      "Feature: 280, Score: 0.39528\n",
      "Feature: 281, Score: -0.61529\n",
      "Feature: 282, Score: -0.25910\n",
      "Feature: 283, Score: -0.07244\n",
      "Feature: 284, Score: 0.00788\n",
      "Feature: 285, Score: -0.26756\n",
      "Feature: 286, Score: -0.13399\n",
      "Feature: 287, Score: 0.77930\n",
      "Feature: 288, Score: 0.39920\n",
      "Feature: 289, Score: 1.25663\n",
      "Feature: 290, Score: -0.05777\n",
      "Feature: 291, Score: 0.47549\n",
      "Feature: 292, Score: -0.54584\n",
      "Feature: 293, Score: 0.50365\n",
      "Feature: 294, Score: -0.90393\n",
      "Feature: 295, Score: -0.20826\n",
      "Feature: 296, Score: -0.43904\n",
      "Feature: 297, Score: -0.04612\n",
      "Feature: 298, Score: -0.01298\n",
      "Feature: 299, Score: 0.46848\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# logistic regression for feature importance\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "X = word_vector_feature #data\n",
    "y = jhj_emoji_labels#target\n",
    "# define the model\n",
    "model = LogisticRegression(max_iter=300)\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# get importance\n",
    "importance = model.coef_[0]\n",
    "#for score,name in zip(importance, uni_vectorizer.get_feature_names()):\n",
    " #   if score < -0.6:\n",
    "        #print(\"feature: \", name, \" score: \", score)\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()\n",
    "\n",
    "#post_sent = numpy.append(post_length, average_sent_len, 1)\n",
    "#post_sent_punct = numpy.append(post_sent, number_of_punct, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature:  es ist  score:  0.614767138681609\n",
      "feature:  ich hab  score:  0.5229178933137119\n",
      "feature:  ich liebe  score:  0.5027960524481465\n",
      "feature:  lust auf  score:  0.5633301295135077\n",
      "feature:  nur noch  score:  0.5008300983285584\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# logistic regression for feature importance\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "X = bigram_feature_vector #data\n",
    "y = jhj_emoji_labels#target\n",
    "# define the model\n",
    "model = LogisticRegression()\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# get importance\n",
    "importance = model.coef_[0]\n",
    "for score,name in zip(importance, bi_vectorizer.get_feature_names()):\n",
    "    if score > 0.5:\n",
    "        print(\"feature: \", name, \" score: \", score)\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3881\n",
      "3881\n",
      "feature:  empfehlen  score:  0.02966237431899547\n",
      "feature:  gibt  score:  0.02867920876154195\n",
      "feature:  jemand  score:  0.12395843054594369\n",
      "feature:  tipps  score:  0.026260855681673648\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#decision tree for feature importance\n",
    "\n",
    "# decision tree for feature importance on a regression problem\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "X = unigram_feature_vector #data\n",
    "y = jhj_emoji_labels#target\n",
    "# define the model\n",
    "model = DecisionTreeRegressor()\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# get importance\n",
    "importance = model.feature_importances_\n",
    "print(len(importance))\n",
    "print(len(uni_vectorizer.get_feature_names()))\n",
    "# summarize feature importance\n",
    "for score,name in zip(importance, uni_vectorizer.get_feature_names()):\n",
    "    if score>0.02:\n",
    "        print(\"feature: \", name, \" score: \", score)\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "3881\n",
      "Feature: 0, Score: 0.17920\n",
      "Feature: 1, Score: 0.04550\n",
      "Feature: 2, Score: 0.19211\n",
      "Feature: 3, Score: 0.19656\n",
      "Feature: 4, Score: 0.27779\n",
      "Feature: 5, Score: 0.10884\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#decision tree for feature importance\n",
    "\n",
    "# decision tree for feature importance on a regression problem\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "X = pos #data\n",
    "y = jhj_emoji_labels#target\n",
    "# define the model\n",
    "model = DecisionTreeRegressor()\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# get importance\n",
    "importance = model.feature_importances_\n",
    "print(len(importance))\n",
    "print(len(uni_vectorizer.get_feature_names()))\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3881\n",
      "Feature: 0, Score: 0.29665\n",
      "Feature: 1, Score: 0.38017\n",
      "Feature: 2, Score: 0.32319\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#decision tree for feature importance\n",
    "\n",
    "# decision tree for feature importance on a regression problem\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "X = post_sent_punct#data\n",
    "y = jhj_emoji_labels#target\n",
    "# define the model\n",
    "model = DecisionTreeRegressor()\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# get importance\n",
    "importance = model.feature_importances_\n",
    "print(len(importance))\n",
    "print(len(uni_vectorizer.get_feature_names()))\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 26\n",
      "[[ 21  17]\n",
      " [  9 125]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.55      0.62        38\n",
      "           1       0.88      0.93      0.91       134\n",
      "\n",
      "    accuracy                           0.85       172\n",
      "   macro avg       0.79      0.74      0.76       172\n",
      "weighted avg       0.84      0.85      0.84       172\n",
      "\n",
      "accuracy score:  0.8488372093023255\n"
     ]
    }
   ],
   "source": [
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = tfidf #data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced')\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.809 (0.033)\n"
     ]
    }
   ],
   "source": [
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = unigram_feature_vector#data\n",
    "y = jhj_emoji_labels#target\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "kf = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "\n",
    "# create model\n",
    "model = LogisticRegression(class_weight='balanced')\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv=kf, n_jobs=-1)\n",
    "\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 35\n",
      "[[  3  35]\n",
      " [  0 134]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.08      0.15        38\n",
      "           1       0.79      1.00      0.88       134\n",
      "\n",
      "    accuracy                           0.80       172\n",
      "   macro avg       0.90      0.54      0.52       172\n",
      "weighted avg       0.84      0.80      0.72       172\n",
      "\n",
      "accuracy score:  0.7965116279069767\n"
     ]
    }
   ],
   "source": [
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = bigram_feature_vector #data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced')\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 35\n",
      "[[  3  35]\n",
      " [  0 134]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.08      0.15        38\n",
      "           1       0.79      1.00      0.88       134\n",
      "\n",
      "    accuracy                           0.80       172\n",
      "   macro avg       0.90      0.54      0.52       172\n",
      "weighted avg       0.84      0.80      0.72       172\n",
      "\n",
      "accuracy score:  0.7965116279069767\n"
     ]
    }
   ],
   "source": [
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = trigram_feature_vector #data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced')\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 25\n",
      "[[ 22  16]\n",
      " [  9 125]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.58      0.64        38\n",
      "           1       0.89      0.93      0.91       134\n",
      "\n",
      "    accuracy                           0.85       172\n",
      "   macro avg       0.80      0.76      0.77       172\n",
      "weighted avg       0.85      0.85      0.85       172\n",
      "\n",
      "accuracy score:  0.8546511627906976\n"
     ]
    }
   ],
   "source": [
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = unigram_bi_feature_vector #data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced')\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 25\n",
      "[[ 19  19]\n",
      " [  6 128]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.50      0.60        38\n",
      "           1       0.87      0.96      0.91       134\n",
      "\n",
      "    accuracy                           0.85       172\n",
      "   macro avg       0.82      0.73      0.76       172\n",
      "weighted avg       0.85      0.85      0.84       172\n",
      "\n",
      "accuracy score:  0.8546511627906976\n"
     ]
    }
   ],
   "source": [
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = uni_bi_tri_feat_vec #data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced')\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 35\n",
      "[[  3  35]\n",
      " [  0 134]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.08      0.15        38\n",
      "           1       0.79      1.00      0.88       134\n",
      "\n",
      "    accuracy                           0.80       172\n",
      "   macro avg       0.90      0.54      0.52       172\n",
      "weighted avg       0.84      0.80      0.72       172\n",
      "\n",
      "accuracy score:  0.7965116279069767\n"
     ]
    }
   ],
   "source": [
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = bi_tri_feat_vec #data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced')\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 25\n",
      "[[ 22  16]\n",
      " [  9 125]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.58      0.64        38\n",
      "           1       0.89      0.93      0.91       134\n",
      "\n",
      "    accuracy                           0.85       172\n",
      "   macro avg       0.80      0.76      0.77       172\n",
      "weighted avg       0.85      0.85      0.85       172\n",
      "\n",
      "accuracy score:  0.8546511627906976\n"
     ]
    }
   ],
   "source": [
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = uni_tri_feat_vec #data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced')\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 76\n",
      "[[21 17]\n",
      " [59 75]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.55      0.36        38\n",
      "           1       0.82      0.56      0.66       134\n",
      "\n",
      "    accuracy                           0.56       172\n",
      "   macro avg       0.54      0.56      0.51       172\n",
      "weighted avg       0.69      0.56      0.60       172\n",
      "\n",
      "accuracy score:  0.5581395348837209\n"
     ]
    }
   ],
   "source": [
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = pos #data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced')\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 31\n",
      "[[ 23  15]\n",
      " [ 16 118]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.61      0.60        38\n",
      "           1       0.89      0.88      0.88       134\n",
      "\n",
      "    accuracy                           0.82       172\n",
      "   macro avg       0.74      0.74      0.74       172\n",
      "weighted avg       0.82      0.82      0.82       172\n",
      "\n",
      "accuracy score:  0.8197674418604651\n"
     ]
    }
   ],
   "source": [
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = uni_pos_new #data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced')\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 33\n",
      "[[  8  30]\n",
      " [  3 131]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.21      0.33        38\n",
      "           1       0.81      0.98      0.89       134\n",
      "\n",
      "    accuracy                           0.81       172\n",
      "   macro avg       0.77      0.59      0.61       172\n",
      "weighted avg       0.79      0.81      0.76       172\n",
      "\n",
      "accuracy score:  0.8081395348837209\n"
     ]
    }
   ],
   "source": [
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = bi_new_pos #data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced')\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 33\n",
      "[[  8  30]\n",
      " [  3 131]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.21      0.33        38\n",
      "           1       0.81      0.98      0.89       134\n",
      "\n",
      "    accuracy                           0.81       172\n",
      "   macro avg       0.77      0.59      0.61       172\n",
      "weighted avg       0.79      0.81      0.76       172\n",
      "\n",
      "accuracy score:  0.8081395348837209\n"
     ]
    }
   ],
   "source": [
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = tri_new_pos #data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced')\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 85\n",
      "[[23 15]\n",
      " [70 64]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.61      0.35        38\n",
      "           1       0.81      0.48      0.60       134\n",
      "\n",
      "    accuracy                           0.51       172\n",
      "   macro avg       0.53      0.54      0.48       172\n",
      "weighted avg       0.69      0.51      0.55       172\n",
      "\n",
      "accuracy score:  0.5058139534883721\n"
     ]
    }
   ],
   "source": [
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = post_sent #data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced')\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 79\n",
      "[[23 15]\n",
      " [64 70]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.61      0.37        38\n",
      "           1       0.82      0.52      0.64       134\n",
      "\n",
      "    accuracy                           0.54       172\n",
      "   macro avg       0.54      0.56      0.50       172\n",
      "weighted avg       0.70      0.54      0.58       172\n",
      "\n",
      "accuracy score:  0.5406976744186046\n"
     ]
    }
   ],
   "source": [
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = post_sent_punct #data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced')\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 64\n",
      "[[25 13]\n",
      " [51 83]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.66      0.44        38\n",
      "           1       0.86      0.62      0.72       134\n",
      "\n",
      "    accuracy                           0.63       172\n",
      "   macro avg       0.60      0.64      0.58       172\n",
      "weighted avg       0.75      0.63      0.66       172\n",
      "\n",
      "accuracy score:  0.627906976744186\n"
     ]
    }
   ],
   "source": [
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = pos_post_sent_len #data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced')\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 59\n",
      "[[24 14]\n",
      " [45 89]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.63      0.45        38\n",
      "           1       0.86      0.66      0.75       134\n",
      "\n",
      "    accuracy                           0.66       172\n",
      "   macro avg       0.61      0.65      0.60       172\n",
      "weighted avg       0.75      0.66      0.68       172\n",
      "\n",
      "accuracy score:  0.6569767441860465\n"
     ]
    }
   ],
   "source": [
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = pos_num #data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced')\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 26\n",
      "[[ 23  15]\n",
      " [ 11 123]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.61      0.64        38\n",
      "           1       0.89      0.92      0.90       134\n",
      "\n",
      "    accuracy                           0.85       172\n",
      "   macro avg       0.78      0.76      0.77       172\n",
      "weighted avg       0.84      0.85      0.85       172\n",
      "\n",
      "accuracy score:  0.8488372093023255\n"
     ]
    }
   ],
   "source": [
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = uni_num #data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced', max_iter = 300)\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 28\n",
      "[[ 22  16]\n",
      " [ 12 122]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.58      0.61        38\n",
      "           1       0.88      0.91      0.90       134\n",
      "\n",
      "    accuracy                           0.84       172\n",
      "   macro avg       0.77      0.74      0.75       172\n",
      "weighted avg       0.83      0.84      0.83       172\n",
      "\n",
      "accuracy score:  0.8372093023255814\n"
     ]
    }
   ],
   "source": [
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X =uni_post_sent_len#data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced', max_iter = 300)\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 43\n",
      "[[  6  32]\n",
      " [ 11 123]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.16      0.22        38\n",
      "           1       0.79      0.92      0.85       134\n",
      "\n",
      "    accuracy                           0.75       172\n",
      "   macro avg       0.57      0.54      0.53       172\n",
      "weighted avg       0.70      0.75      0.71       172\n",
      "\n",
      "accuracy score:  0.75\n"
     ]
    }
   ],
   "source": [
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = bi_num #data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced', max_iter=300)\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 39\n",
      "[[ 14  24]\n",
      " [ 15 119]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.37      0.42        38\n",
      "           1       0.83      0.89      0.86       134\n",
      "\n",
      "    accuracy                           0.77       172\n",
      "   macro avg       0.66      0.63      0.64       172\n",
      "weighted avg       0.75      0.77      0.76       172\n",
      "\n",
      "accuracy score:  0.7732558139534884\n"
     ]
    }
   ],
   "source": [
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = tri_post_sent_pos #data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced', max_iter = 300)\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 25\n",
      "[[ 29   9]\n",
      " [ 16 118]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.70        38\n",
      "           1       0.93      0.88      0.90       134\n",
      "\n",
      "    accuracy                           0.85       172\n",
      "   macro avg       0.79      0.82      0.80       172\n",
      "weighted avg       0.87      0.85      0.86       172\n",
      "\n",
      "accuracy score:  0.8546511627906976\n"
     ]
    }
   ],
   "source": [
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = word_vector_feature #data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced', max_iter=400)\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 30\n",
      "[[ 26  12]\n",
      " [ 18 116]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.68      0.63        38\n",
      "           1       0.91      0.87      0.89       134\n",
      "\n",
      "    accuracy                           0.83       172\n",
      "   macro avg       0.75      0.77      0.76       172\n",
      "weighted avg       0.84      0.83      0.83       172\n",
      "\n",
      "accuracy score:  0.8255813953488372\n"
     ]
    }
   ],
   "source": [
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = uni_wordvector #data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced', max_iter=300)\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 12\n",
      "[[ 35   3]\n",
      " [  9 125]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.92      0.85        38\n",
      "           1       0.98      0.93      0.95       134\n",
      "\n",
      "    accuracy                           0.93       172\n",
      "   macro avg       0.89      0.93      0.90       172\n",
      "weighted avg       0.94      0.93      0.93       172\n",
      "\n",
      "accuracy score:  0.9302325581395349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = tri_wordvector #data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced')\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wordvector_pos_num' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5538540d5f21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#unigrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordvector_pos_num\u001b[0m \u001b[0;31m#data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjhj_emoji_labels\u001b[0m\u001b[0;31m#target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wordvector_pos_num' is not defined"
     ]
    }
   ],
   "source": [
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = wordvector_pos_num #data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced')\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 37\n",
      "[[ 25  13]\n",
      " [ 24 110]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.66      0.57        38\n",
      "           1       0.89      0.82      0.86       134\n",
      "\n",
      "    accuracy                           0.78       172\n",
      "   macro avg       0.70      0.74      0.72       172\n",
      "weighted avg       0.81      0.78      0.79       172\n",
      "\n",
      "accuracy score:  0.7848837209302325\n"
     ]
    }
   ],
   "source": [
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = pos_bi_count_vec #data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced')\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 53\n",
      "[[24 14]\n",
      " [39 95]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.63      0.48        38\n",
      "           1       0.87      0.71      0.78       134\n",
      "\n",
      "    accuracy                           0.69       172\n",
      "   macro avg       0.63      0.67      0.63       172\n",
      "weighted avg       0.76      0.69      0.71       172\n",
      "\n",
      "accuracy score:  0.6918604651162791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = pos_uni_count_vec #data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced')\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 52\n",
      "[[25 13]\n",
      " [39 95]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.66      0.49        38\n",
      "           1       0.88      0.71      0.79       134\n",
      "\n",
      "    accuracy                           0.70       172\n",
      "   macro avg       0.64      0.68      0.64       172\n",
      "weighted avg       0.77      0.70      0.72       172\n",
      "\n",
      "accuracy score:  0.6976744186046512\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = tfidf_pos #data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced')\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 24\n",
      "[[ 29   9]\n",
      " [ 15 119]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71        38\n",
      "           1       0.93      0.89      0.91       134\n",
      "\n",
      "    accuracy                           0.86       172\n",
      "   macro avg       0.79      0.83      0.81       172\n",
      "weighted avg       0.87      0.86      0.86       172\n",
      "\n",
      "accuracy score:  0.8604651162790697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = wordvector_tfidf  #data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced')\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 23\n",
      "[[ 30   8]\n",
      " [ 15 119]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.79      0.72        38\n",
      "           1       0.94      0.89      0.91       134\n",
      "\n",
      "    accuracy                           0.87       172\n",
      "   macro avg       0.80      0.84      0.82       172\n",
      "weighted avg       0.88      0.87      0.87       172\n",
      "\n",
      "accuracy score:  0.8662790697674418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = word_vector_feature #data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced')\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 32\n",
      "[[ 24  14]\n",
      " [ 18 116]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.63      0.60        38\n",
      "           1       0.89      0.87      0.88       134\n",
      "\n",
      "    accuracy                           0.81       172\n",
      "   macro avg       0.73      0.75      0.74       172\n",
      "weighted avg       0.82      0.81      0.82       172\n",
      "\n",
      "accuracy score:  0.813953488372093\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X = word_vector_feature #data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "\n",
    "dec_tree = DecisionTreeClassifier(class_weight='balanced')\n",
    "y_pred = dec_tree.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS AS FEATURES\n",
      "Number of mislabeled points out of a total 172 points : 32\n",
      "[[ 24  14]\n",
      " [ 18 116]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.63      0.60        38\n",
      "           1       0.89      0.87      0.88       134\n",
      "\n",
      "    accuracy                           0.81       172\n",
      "   macro avg       0.73      0.75      0.74       172\n",
      "weighted avg       0.82      0.81      0.82       172\n",
      "\n",
      "accuracy score:  0.813953488372093\n"
     ]
    }
   ],
   "source": [
    "#test balanced class weights\n",
    "\n",
    "#unigrams\n",
    "X =unigram_feature_vector#data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logregr = LogisticRegression(class_weight='balanced', max_iter = 300)\n",
    "y_pred = logregr.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#evaluation\n",
    "print(\"UNIGRAMS AS FEATURES\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############tuning############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:  0.9533799533799534\n",
      "improved\n",
      "Number of mislabeled points out of a total 172 points : 29\n",
      "[[ 22  16]\n",
      " [ 13 121]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.58      0.60        38\n",
      "           1       0.88      0.90      0.89       134\n",
      "\n",
      "    accuracy                           0.83       172\n",
      "   macro avg       0.76      0.74      0.75       172\n",
      "weighted avg       0.83      0.83      0.83       172\n",
      "\n",
      "accuracy score:  0.8313953488372093\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#unigrams\n",
    "X = unigram_feature_vector#data\n",
    "y = jhj_emoji_labels#target\n",
    "\n",
    "#seperate test and training ->20%test 80%training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#train and fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "improved_model = LogisticRegression(class_weight='balanced', penalty='l2', C=0.5)\n",
    "y_pred = improved_model.fit(X_train, y_train).predict(X_test)\n",
    "print(\"score: \", improved_model.score(X,y))\n",
    "#evaluation\n",
    "print(\"improved\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"accuracy score: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "cm = confusion_matrix(y, improved_model.predict(X))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.imshow(cm)\n",
    "ax.grid(False)\n",
    "ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "ax.set_ylim(1.5, -0.5)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "########plotting#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_values = [numpy.min(X[:, 1] - 5), numpy.max(X[:, 2] + 5)]\n",
    "y_values = - (y_pred[0] + numpy.dot(y_pred[1], x_values)) / y_pred[2]\n",
    "\n",
    "#x_values =X\n",
    "#y_values =y\n",
    "plt.plot(x_values, y_values, label='Decision Boundary')\n",
    "plt.xlabel('Marks in 1st Exam')\n",
    "plt.ylabel('Marks in 2nd Exam')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,) (172,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-b9f7ccbc8d63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m50\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m50\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0my_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_values\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'red'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'blue'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Logistics regression'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,) (172,) "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x504 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Plotting the decision boundary\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "x_values = [np.min(X_train[:,] -50 ), np.max(X_train[:,] +50 )]\n",
    "y_values = np.dot((-1./y_pred[1]), (np.dot(y_pred[0],x_values) + y_pred))\n",
    "colors=['red' if l==0 else 'blue' for l in y_train]\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], label='Logistics regression', color=colors)\n",
    "plt.plot(x_values, y_values, label='Decision Boundary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
